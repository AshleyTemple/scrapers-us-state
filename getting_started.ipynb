{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up your development environ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Clone the state scraper repo from Github\n",
    "\n",
    "  ```\n",
    "  git clone https://github.com/influence-usa/scrapers-us-state.git\n",
    "  ```\n",
    "\n",
    "2. Make a new virtualenv\n",
    "\n",
    "  ```\n",
    "  mkvirtualenv --python=$(which python3)\n",
    "  ```\n",
    "  \n",
    "3. Use `pip` to install requirements\n",
    "\n",
    "  ```\n",
    "  pip install -r requirements.txt\n",
    "  ```\n",
    "  \n",
    "4. If you don't see a folder for the state you're working on, run the following:\n",
    "\n",
    "  ```\n",
    "    (iusa-scrape)$>pupa init arizona\n",
    "    no pupa_settings on path, using defaults\n",
    "    jurisdiction name (e.g. City of Seattle): Arizona\n",
    "    division id (e.g. ocd-division/country:us/state:wa/place:seattle): ocd-division/country:us/state:az\n",
    "    classification (can be: government, legislature, executive, school_system): government\n",
    "    official URL: http://www.az.gov/\n",
    "    create disclosures scraper? [Y/n]: Y\n",
    "    create bills scraper? [y/N]: n\n",
    "    create events scraper? [y/N]: y\n",
    "    create votes scraper? [y/N]: n\n",
    "    create people scraper? [y/N]: y\n",
    "  ```\n",
    "\n",
    "...what this did was create a new folder for the state. In this example, the state was Arizona (`arizona`).\n",
    "\n",
    "  ```\n",
    "    (iusa-scrape)$>tree\n",
    "    .\n",
    "    ├── ak\n",
    "    │   └── __init__.py\n",
    "    ├── al\n",
    "    │   ├── __init__.py\n",
    "    │   └── people.py\n",
    "    ├── arizona\n",
    "    │   ├── disclosures.py\n",
    "    │   ├── events.py\n",
    "    │   ├── __init__.py\n",
    "    │   └── people.py\n",
    "    ├── md\n",
    "    │   └── __init__.py\n",
    "    ├── README.md\n",
    "    ├── requirements.txt\n",
    "    ├── Untitled.ipynb\n",
    "    └── utils\n",
    "        ├── __init__.py\n",
    "        └── lxmlize.py\n",
    "  ```\n",
    "  \n",
    "To follow the broader pupa convention, we'll change the directory name to `az`:\n",
    "\n",
    "```\n",
    "    (iusa-scrape)$>mv arizona az\n",
    "    (iusa-scrape)$>tree\n",
    "    tree\n",
    "    .\n",
    "    ├── ak\n",
    "    │   └── __init__.py\n",
    "    ├── al\n",
    "    │   ├── __init__.py\n",
    "    │   └── people.py\n",
    "    ├── az\n",
    "    │   ├── disclosures.py\n",
    "    │   ├── events.py\n",
    "    │   ├── __init__.py\n",
    "    │   └── people.py\n",
    "    ├── md\n",
    "    │   └── __init__.py\n",
    "    ├── README.md\n",
    "    ├── requirements.txt\n",
    "    ├── Untitled.ipynb\n",
    "    └── utils\n",
    "        ├── __init__.py\n",
    "        └── lxmlize.py\n",
    "\n",
    "5 directories, 13 files\n",
    "```\n",
    "  \n",
    "Because we told it to in teh questions asked above, it also created the starter code for our scrapers: there's one each for disclosures, events and people.\n",
    "\n",
    "Also interesting is the `__init__.py` file in our state's directory.  It used the answers to our questions to build a `Jurisdiction` object that represents the state government:\n",
    "\n",
    "```\n",
    "class Arizona(Jurisdiction):\n",
    "    division_id = \"ocd-division/country:us/state:az\"\n",
    "    classification = \"government\"\n",
    "    name = \"Arizona\"\n",
    "    url = \"https://az.gov/\"\n",
    "    scrapers = {\n",
    "        \"events\": ArizonaEventScraper,\n",
    "        \"people\": ArizonaPersonScraper,\n",
    "        \"disclosures\": ArizonaDisclosureScraper,\n",
    "    }\n",
    "\n",
    "    def get_organizations(self):\n",
    "        yield Organization(name=None, classification=None)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Version control"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a good time to add and commit our changes so far.\n",
    "\n",
    "```\n",
    "(iusa-scrape)$>git add az\n",
    "(iusa-scrape)$>git commit -m \"initialized arizona\"\n",
    "[master 3e622ef] initialized arizona\n",
    " 4 files changed, 47 insertions(+)\n",
    " create mode 100644 az/__init__.py\n",
    " create mode 100644 az/disclosures.py\n",
    " create mode 100644 az/events.py\n",
    " create mode 100644 az/people.py\n",
    " ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Where is the data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating global authority organizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Secretary of State"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `get_organizations` method of the `Jurisdiction` class lets us define some global organizations for all of the data that we'll be scraping from Arizona's sites. For campaign finance disclosures, we'll have to define the Arizona Secretary of State's Office.\n",
    "\n",
    "```\n",
    "def get_organizations(self):                                        \n",
    "```\n",
    "\n",
    "First, initialize using the `Organization` class.\n",
    "\n",
    "```\n",
    "    secretary_of_state = Organization(                                    \n",
    "        name=\"Office of the Secretary of State, State of Arizona\",        \n",
    "        classification=\"office\"                                           \n",
    "    )                                                               ```\n",
    "    \n",
    "Here, we're able to set particular attributes using `kwargs`.  To get a sense of which attributes you can set at this point, check out the [source](https://github.com/influence-usa/pupa/blob/disclosures/pupa/scrape/popolo.py#L132-L182).\n",
    "\n",
    "Now, we can add other attribtues, using the helper methods found on the `Organization` class:\n",
    "\n",
    "```\n",
    "    secretary_of_state.add_contact_detail(                                \n",
    "        type=\"voice\",                                                     \n",
    "        value=\"602-542-4285\"                                              \n",
    "    )                    \n",
    "\n",
    "    secretary_of_state.add_contact_detail(                                \n",
    "        type=\"address\",                                                   \n",
    "        value=\"1700 W Washington St Fl 7, Phoenix AZ 85007-2808\"          \n",
    "    )                                                                     \n",
    "    secretary_of_state.add_link(                                          \n",
    "        url=\"http://www.azsos.gov/\",                                      \n",
    "        note=\"Home page\"                                                  \n",
    "    )                                                                     \n",
    "```\n",
    "\n",
    "We should add the organization we've created to the `Jurisdiction` object as a semi-private property. This is useful, beacuse the `Jurisdiction` object will essentially always be accessible to all of our scrapers. Whenever we want to refer to the AZ Secretary of State, we can always access it from `Arizona` jurisdiction object.\n",
    "\n",
    "```\n",
    "    self._secretary_of_state = secretary_of_state                   \n",
    "```\n",
    "\n",
    "Finally, yield the organization we created. This is beacause `get_organizations` is actually the first scraper that we'll run each time we run Arizona scrapers of any kind.\n",
    "\n",
    "```\n",
    "    yield secretary_of_state                                          \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test what we have so far!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool, let's try out what we have so far.  From the project root (`scrapers-us-state`), run the command:\n",
    "\n",
    "```\n",
    "(iusa-scrape)$>pupa update az --scrape\n",
    "```\n",
    "\n",
    "This will throw a `ScrapeError` because we haven't written any of the main scrapers yet, but before it does we'll see that it creates our `Jurisdiction` object, and the `Organization` representing teh Arizona Secretary of State.\n",
    "\n",
    "```\n",
    "no pupa_settings on path, using defaults\n",
    "az (scrape)\n",
    "  events: {}\n",
    "  people: {}\n",
    "  disclosures: {}\n",
    "Not checking sessions...\n",
    "13:30:10 INFO pupa: save jurisdiction Arizona as jurisdiction_ocd-jurisdiction-country:us-state:az-government.json\n",
    "13:30:10 INFO pupa: save organization Office of the Secretary of State, State of Arizona as organization_1e330580-e20b-11e4-a4f5-e90fe0697b56.json\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Starting a new scraper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to start writing the real meat and potatoes of our scraping code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Locate the source of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check out the [Big Board](https://docs.google.com/spreadsheets/d/18-MvVJXg8TkUUNhtBmWoCEPUWEMf7F6-YVV6x7CWrg4/pubhtml) to see which URL you should use to start. Explore the links on that page until you find the data you're looking for.\n",
    "\n",
    "For this example, we'll look at the Arizona Super PAC list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PAC_LIST_URL = \"http://apps.azsos.gov/apps/election/cfs/search/SuperPACList.aspx\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding new scrape routines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to add our code to `az/disclosures.py`. \n",
    "\n",
    "```\n",
    "class ArizonaDisclosureScraper(Scraper):\n",
    "                                           \n",
    "    def scrape_super_pacs(self):           \n",
    "        pass                               \n",
    "                                           \n",
    "    def scrape(self):                      \n",
    "        # needs to be implemented          \n",
    "        yield from self.scrape_super_pacs()\n",
    "```\n",
    "\n",
    "When we're through, the `pupa` CLI commands will call the `scrape` command. It's good practice to follow this pattern to break down that command into a series of subroutines, one for each type of data you're returning. The pupa software actually doesn't care, though, it just expects a stream of Open Civic Data scrape objects (`Person`, `Organizaton`, `Event`, etc)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Developing your scraper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, you might want to move to a REPL (or, even better, to an IPython notebook) so that you can start figuring out how to obtain the target data. You'll \n",
    "\n",
    "In this example, things are fairly straightforward.  There's a `<table>` element in the middle of the page that has all the information we need to generate the `Organization` objects that we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from lxml import etree\n",
    "from lxml.html import HTMLParser\n",
    "\n",
    "import scrapelib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "resp = scrapelib.urlopen('http://localhost:8000/SuperPACList.aspx.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d = etree.fromstring(resp, parser=HTMLParser())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The easiest thing to do is just look for the table we're interested by writing an xpath query. The `<table>` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Element table at 0x7fbe9c435138>, <Element table at 0x7fbe9c435188>]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.xpath('//table')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hm, looks like there's more than one, so we're going to have to narrow our XPath query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
